{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file and return processed json list\n",
    "def process_json(json_data_dir):\n",
    "\n",
    "    with open(json_data_dir, \"r\") as json_file:\n",
    "        \n",
    "        lines = json_file.readlines() # each line contains one json object as str\n",
    "        \n",
    "        json_list = [] # a list of dictonaries, where each dictionary is a json object\n",
    "\n",
    "        for l in lines:\n",
    "            data = json.loads(l)\n",
    "\n",
    "            # ucid = data['ucid']\n",
    "            \n",
    "            # invention_title = data['invention_title']['text']\n",
    "            abstract = data['abstract']['text']\n",
    "            # claims = data['claims']['text']\n",
    "\n",
    "            # invention_title_processed = process_invention_title(invention_title)\n",
    "            abstract_processed = process_abstract(abstract)\n",
    "\n",
    "            # claims_processed = process_claims(claims)\n",
    "\n",
    "            # final_text = invention_title_processed + ' ' + abstract_processed + ' ' + claims_processed\n",
    "        \n",
    "            json_list.append({'abstract':abstract_processed})\n",
    "            \n",
    "    \n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv file\n",
    "def write2csv(json_dict, output_dir):\n",
    "    with open(output_dir, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        field_names = ['abstract']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=field_names)\n",
    "        # writer.writeheader()\n",
    "        writer.writerows(json_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process XML Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_invention_title(text):\n",
    "    xml = ET.fromstring(text)\n",
    "    invention_title = preprocess(xml.text)\n",
    "    return invention_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_within_tags(element):\n",
    "    text = element.text or ''\n",
    "    for child in element:\n",
    "        text += ET.tostring(child, encoding='unicode', method='text')\n",
    "        if child.tail:\n",
    "            text += child.tail\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_abstract(text):\n",
    "    p_tag_content = \"\"\n",
    "    xml = ET.fromstring(text)\n",
    "    p_tags = xml.findall('.//p')\n",
    "    for p in p_tags:\n",
    "        p_tag_content += get_text_within_tags(p)\n",
    "\n",
    "    processed = preprocess(p_tag_content)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_claims(claims_text):\n",
    "    text = \"\"\n",
    "    xml = ET.fromstring(claims_text)\n",
    "\n",
    "    # extract claims text out of xml\n",
    "    for claims in xml:\n",
    "        for claim in claims:\n",
    "            if claim.text is not None:\n",
    "                text += claim.text\n",
    "\n",
    "    text_cleaned = preprocess(text)\n",
    "    \n",
    "    return text_cleaned    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # Normalize Unicode characters\n",
    "    # text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    # print(\"Normalized Text:\", text)\n",
    "\n",
    "\n",
    "    # Remove newline characters\n",
    "    text = text.replace('\\n', '').replace('\\r', '')\n",
    "    text_cleaned = text\n",
    "    # print('newline removed: ', text)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # # Lowercasing\n",
    "    # text = text.lower()\n",
    "\n",
    "    # # Removing HTML tags\n",
    "    # text = re.sub(r'<.*?>', '', text)\n",
    "    # print(\"html_removed: \", text)\n",
    "\n",
    "    # # Tokenization\n",
    "\n",
    "    # pattern = r'\\b\\w+(?:-\\w+)*\\b' # \\boundary \\word (-\\word)* \\boundary\n",
    "    # tokens = regexp_tokenize(text, pattern)\n",
    "    # print(\"tokenized: \", tokens)\n",
    "\n",
    "    # Remove punctuation except dashed-words\n",
    "    # tokens = [re.sub(r'[^\\w-]', '', word) for word in tokens]\n",
    "    # print(\"punc removed: \", tokens)\n",
    "    \n",
    "    # Remove numbers\n",
    "    # tokens = [word for word in tokens if not word.isdigit()]\n",
    "    # print(\"num removed: \", tokens)\n",
    "\n",
    "    # # Remove stop words\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [word for word in tokens if word not in stop_words]\n",
    "    # print(\"stop words removed: \", tokens)\n",
    "\n",
    "    # Remove empty strings\n",
    "    # tokens = [word for word in tokens if word != \"\"]\n",
    "    # print(\"empty strings removed: \", tokens)\n",
    "\n",
    "    # # Lemmatization\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # print(\"lematized: \", tokens)\n",
    "\n",
    "\n",
    "    # Join the tokens back\n",
    "    # text_cleaned = ' '.join(tokens)\n",
    "    # print(\"final: \", text_cleaned)\n",
    "\n",
    "    return text_cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_data = process_json('./data/sample_data_2.json')\n",
    "write2csv(collect_data, './output_data/abstracts_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
